{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing term / entity distributions in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.archive.archivespark._\n",
    "import org.archive.archivespark.functions._\n",
    "import org.archive.archivespark.specific.warc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this example, the web archive dataset will be loaded from local WARC / CDX files (created in [this recipe](Downloading_WARC_from_Wayback.ipynb)). However, any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) could be used here too, in order to load your records of different types and from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val warcPath = \"/data/helgeholzmann-de.warc.gz\"\n",
    "val cdxPath = warcPath + \"/*.cdx.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcSpec.fromFiles(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering records\n",
    "\n",
    "Embeds are specific to webpages, so we can filter out videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), as well as webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200).\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first record in our remaining dataset, we can see that this indeed is of type *text/html* and was *online* (status 200) at the time of crawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152652\",\n",
       "        \"digest\" : \"sha1:HCHVDRUSN7WDGNZFJES2Y4KZADQ6KINN\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2087,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL deduplication\n",
    "\n",
    "As we want to consider the number of URLs mentioning a term as the term's frequency, we first need to make sure, every URL is only included once in the dataset. Therefore, we simply decide for the earliest snapshot of each URL. This should be cached to avoid recompution every time a record is accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val earliest = pages.distinctValue(_.surtUrl) {(a, b) => if (a.time.isBefore(b.time)) a else b}.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Counting terms\n",
    "\n",
    "To extract the terms of a webpage, we have to keep in mind that a webpage consists of HTML code. Hence, using the `StringContent` enrichment function would enrich our dataset with this HTML code. To parse the HTML and only keep the text, we provide the `HtmlText` enrichment function. This can be used to extract the text of a single tag, e.g., `HtmlText.of(Html.first(\"title\"))` to get the title text of a page. By default though, `HtmlText` extracts the entire text of the page.\n",
    "\n",
    "For more details on the [Enrichment Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) provided with ArchiveSpark and their usage, please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528153841\",\n",
       "        \"digest\" : \"sha1:M3YAC4HWZEWOUBTUWVZG6TLTOCBAFX7G\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/contact\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/contact\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2001,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"html\" : {\n",
       "                    \"text\" : \"Helge Holzmann - @helgeho Home Research Publications Private Projects Contact Helge Holzmann I am a researcher and PhD candidate at the L3S Research Center in Hannover, Germany. My main research interest is on Web archives and related topics, such as big data processing, graph analysis..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(HtmlText).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn text into terms\n",
    "\n",
    "As a very simple normalization, we convert the text into lowercase, before we split it up into single distinct terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Terms = LowerCase.of(HtmlText).mapMulti(\"terms\")(text => text.split(\"\\\\W+\").distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528153841\",\n",
       "        \"digest\" : \"sha1:M3YAC4HWZEWOUBTUWVZG6TLTOCBAFX7G\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/contact\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/contact\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2001,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"html\" : {\n",
       "                    \"text\" : {\n",
       "                        \"lowercase\" : {\n",
       "                            \"terms\" : [\n",
       "                                \"helge\",\n",
       "                                \"holzmann\",\n",
       "                                \"helgeho\",\n",
       "                                \"home\",\n",
       "                                \"rese..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(Terms).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute term frequencies (number of records / URLs)\n",
    "\n",
    "We can now use `.flatMapValues` to get a plain list of the terms included in the dataset. To get rid of short stopwords like articles, we only keep those terms with a minimum length of 4 characters.\n",
    "\n",
    "For more details on available [ArchiveSpark operations](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md), please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val terms = earliest.flatMapValues(Terms).filter(_.length >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "helge\n",
      "holzmann\n",
      "helgeho\n",
      "home\n",
      "research\n",
      "publications\n",
      "private\n",
      "projects\n",
      "contact\n",
      "researcher\n"
     ]
    }
   ],
   "source": [
    "terms.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we made sure before that every URL is included only once in the dataset and each term is included only once per record, we can simply count the terms, using Spark's `.countByValue`. Finally, we sort the terms by count in descending order (negative count) and save them as [CSV (comma-separated values)](https://en.wikipedia.org/wiki/Comma-separated_values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts = terms.countByValue.toSeq.sortBy{case (term, count) => -count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(researcher,3)\n",
      "(reserved,3)\n",
      "(github,3)\n",
      "(topics,3)\n",
      "(holzmann,3)\n",
      "(germany,3)\n",
      "(private,3)\n",
      "(arxiv,3)\n",
      "(research,3)\n",
      "(email,3)\n"
     ]
    }
   ],
   "source": [
    "counts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saving the CSV file we can use `IOUtil.writeLines`, which is included with the *Sparkling* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.archive.archivespark.sparkling.io._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IOUtil.writeLines(\"term_counts.csv\", counts.map{case (term, count) => term + \",\" + count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `term_counts.csv` that is created in the same folder as this notebook will contain all terms. Now this CSV file can be loaded in a plotting tool of your choice, for example to plot a historgram of the term distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting named entities\n",
    "\n",
    "Similar to the term frequencies as shown above we can also count the occurrences of [named entities](https://en.wikipedia.org/wiki/Named_entity) in the dataset.\n",
    "\n",
    "An [Enrichment Function](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) to extract named entities is provided with ArchiveSpark, named `Entities`. It uses [Stanford's CoreNLP](https://stanfordnlp.github.io/CoreNLP/) Named Entity Extractor. In order to use it you need to add [`edu.stanford.nlp:stanford-corenlp:3.5.1`](http://central.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.5.1/) with corresponding models to your classpath.\n",
    "\n",
    "Another Enrichment Function that uses [Yahoo's Fast Entity Linker](https://github.com/yahoo/FEL) (FEL) for more accurate Entity Linking with ArchiveSpark can be found here: [FEL4ArchiveSpark](https://github.com/helgeho/FEL4ArchiveSpark)\n",
    "\n",
    "For more details on the [Enrichment Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) and their use, please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528153841\",\n",
       "        \"digest\" : \"sha1:M3YAC4HWZEWOUBTUWVZG6TLTOCBAFX7G\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/contact\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/contact\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2001,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"html\" : {\n",
       "                    \"entities\" : {\n",
       "                        \"persons\" : [\n",
       "                            \"Helge\",\n",
       "                            \"Holzmann\"\n",
       "                        ],\n",
       "                        \"organizations\" : [\n",
       "                        ],\n",
       "                        \"locations\" : [\n",
       "                            \"..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(Entities).peekJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val locations = earliest.flatMapValues(Entities.child[Seq[String]](\"locations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Hannover\n",
      "Germany\n",
      "MA\n",
      "Pisa\n",
      "USA\n",
      "London\n",
      "Greece\n",
      "Quebec\n",
      "Valetta\n",
      "Boston\n"
     ]
    }
   ],
   "source": [
    "locations.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note:*\n",
    "\n",
    "*Named Entity Extraction is a pretty expensive operation, depending on the size of the dataset, the following instruction may run for hours or even days.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts = locations.countByValue.toSeq.sortBy{case (term, count) => -count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hannover,3)\n",
      "(Germany,3)\n",
      "(MA,1)\n",
      "(Pisa,1)\n",
      "(USA,1)\n",
      "(London,1)\n",
      "(Greece,1)\n",
      "(Quebec,1)\n",
      "(Valetta,1)\n",
      "(Boston,1)\n"
     ]
    }
   ],
   "source": [
    "counts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IOUtil.writeLines(\"location_counts.csv\", counts.map{case (term, count) => term + \",\" + count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "The use of `.countByValue` automatically fetches / collects the counts for all available values to the local driver, which may lead to memory issues if the dataset is too big. Instead, the same operation can be implemented by a distributed `.reduceByKey` operation, with a filter to ensure that only values with high counts are fetched in order to avoid memory overruns. This way, also the sorting can be achieved in a distributed fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val termCounts = terms.map(term => (term, 1L)).reduceByKey(_ + _).filter{case (term, count) => count > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fetchedTermCounts = termCounts.sortBy{case (term, count) => -count}.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(helgeho,3)\n",
      "(main,3)\n",
      "(interest,3)\n",
      "(arxiv,3)\n",
      "(center,3)\n",
      "(data,3)\n",
      "(hannover,3)\n",
      "(home,3)\n",
      "(archives,3)\n",
      "(email,3)\n"
     ]
    }
   ],
   "source": [
    "fetchedTermCounts.take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark",
   "language": "",
   "name": "archivespark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
