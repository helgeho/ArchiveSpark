{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading WARC / Generating CDX (enabling more efficient processing)\n",
    "\n",
    "ArchiveSpark gains its efficiency through a two-step loading approach, which only accesses metadata for common operations like filtering, sorting, grouping, etc. Only if content is required for applying additional filters or derive new information from a record, ArchiveSpark will access the actual records. The required metadata for web archives is commonly provided by CDX records. In the following we show how to generate these CDX records from a collection of (W)ARC(.gz) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.archive.webservices.archivespark._\n",
    "import org.archive.webservices.archivespark.functions._\n",
    "import org.archive.webservices.archivespark.specific.warc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from (W)ARC(.gz) files (without CDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the web archive dataset will be loaded from local WARC files only (created in [this recipe](Downloading_WARC_from_Wayback.ipynb)), without the corresponding CDX files. This is a lot slower than using CDX metadata records, but sometimes necessary if CDX files are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val warc = ArchiveSpark.load(WarcSpec.fromFiles(\"/data/helgeholzmann-de.warc.gz/*.warc.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a look at the first record\n",
    "\n",
    "As we can see, although loaded directly from WARC, the records are internally represented in the same format as datasets with provided CDX data. Hence, we can apply the same operations as well Enrichment Functions, however, the processing will be less efficient than with available CDX records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152652\",\n",
       "        \"digest\" : \"sha1:HCHVDRUSN7WDGNZFJES2Y4KZADQ6KINN\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2087,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warc.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the records in this dataset takes long as all headers and contents are read and parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warc.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(this can take hours / days for large datasets)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating CDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate and save the CDX records corresponding to our dataset for a more efficient use of this dataset with ArchiveSpark in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warc.saveAsCdx(\"/data/helgeholzmann-de.cdx.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by adding .gz to the path, the output will automatically be compressed using GZip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-loading dataset with CDX records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have CDX records for our dataset now, we can load it more efficiently by providing the CDX location to use a suitable [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcSpec.fromFiles(\"/data/helgeholzmann-de.cdx.gz\", \"/data/helgeholzmann-de.warc.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting as well as most of the other [operations](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md) provided by [Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html) as well as [ArchiveSpark](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md) will be more efficient now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(this usually takes seconds / minutes, depending on the size of the dataset)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark",
   "language": "",
   "name": "archivespark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
