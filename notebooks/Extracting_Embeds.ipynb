{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting embedded resources from webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.archive.webservices.archivespark._\n",
    "import org.archive.webservices.archivespark.functions._\n",
    "import org.archive.webservices.archivespark.specific.warc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this example, the web archive dataset will be loaded from local WARC / CDX files (created in [this recipe](Downloading_WARC_from_Wayback.ipynb)). However, any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) could be used here too, in order to load your records of different types and from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val warcPath = \"/data/helgeholzmann-de.warc.gz\"\n",
    "val cdxPath = warcPath + \"/*.cdx.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcSpec.fromFiles(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering irrelevant records\n",
    "\n",
    "Embeds are specific to webpages, so we can filter out videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), as well as webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200).\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first record in our remaining dataset, we can see that this indeed is of type *text/html* and was *online* (status 200) at the time of crawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152652\",\n",
       "        \"digest\" : \"sha1:HCHVDRUSN7WDGNZFJES2Y4KZADQ6KINN\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2087,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates\n",
    "\n",
    "In order to save processing time, we remove duplicate websites (based on the digest in the CDX records) and only keep the earliest snapshot for each distinct content. This will be cached, so that we do not need to compute it every time we want to access that collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val earliest = pages.distinctValue(_.digest) {(a, b) => if (a.time.isBefore(b.time)) a else b}.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting embedded resources\n",
    "\n",
    "In this example we want to extract stylesheets, hence we are interested in `link` tags with attribute `rel=\"stylesheet\"`. Similarly, we could also extract images or other resources.\n",
    "\n",
    "We first need to define the required [Enrichment Function](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) to enrich our metadata with the URLs (in SURT format) of the embedded stylesheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Stylesheets = Html.all(\"link\").mapMulti(\"stylesheets\") { linkTags => linkTags.filter(_.contains(\"rel=\\\"stylesheet\\\"\"))}\n",
    "val StylesheetUrls = SURT.of(HtmlAttribute(\"href\").ofEach(Stylesheets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152831\",\n",
       "        \"digest\" : \"sha1:XRVCBHVKAC6NQ4N24OCF4S2ABYUOJW3H\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/publications\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/publications\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 4280,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"link\" : {\n",
       "                    \"stylesheets\" : [\n",
       "                        {\n",
       "                            \"attributes\" : {\n",
       "                                \"href\" : {\n",
       "                                    \"SURT\" : \"de,helgeholzmann)/images/favicon.png\"\n",
       "                                }\n",
       "                            }\n",
       "       ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(StylesheetUrls).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the relevant embeds / stylesheets in the dataset\n",
    "\n",
    "At this point, we have to access the original dataset `records` again, as the stylesheets are not among the filtered `pages`.\n",
    "A `join` operation is used to filter the records in the dataset and keep only the previously extracted stylesheet files. As a `join` is performed on the keys in the dataset, we introduce a dummy value (`true`) here to make the URL the key of the records. For more information please read the [Spark Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stylesheetUrls = earliest.flatMapValues(StylesheetUrls.multi).distinct.map(url => (url, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stylesheets = records.map(r => (r.surtUrl, r)).join(stylesheetUrls).map{case (url, (record, dummy)) => record}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we again remove duplicates in the stylesheet dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinctStylesheets = stylesheets.distinctValue(_.digest) {(a, b) => if (a.time.isBefore(b.time)) a else b}.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152655\",\n",
       "        \"digest\" : \"sha1:ETFODABRIVFG5WELMU2Y3UE7U66RQXD5\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/css/academicons.min.css\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/css/academicons.min.css\",\n",
       "        \"mime\" : \"text/css\",\n",
       "        \"compressedSize\" : 1497,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctStylesheets.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving the relevant embeds\n",
    "\n",
    "There are different options to save the embeds datasets. One way would be to save the embeds as WARC records as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctStylesheets.saveAsWarc(\"stylesheets.warc.gz\", WarcMeta(publisher = \"Internet Archive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to enrich the metadata of the stylesheets with their actual content and save it as JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val enriched = distinctStylesheets.enrich(StringContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152655\",\n",
       "        \"digest\" : \"sha1:ETFODABRIVFG5WELMU2Y3UE7U66RQXD5\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/css/academicons.min.css\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/css/academicons.min.css\",\n",
       "        \"mime\" : \"text/css\",\n",
       "        \"compressedSize\" : 1497,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : \"@font-face{font-family:'Academicons';src:url('../fonts/academicons.eot?v=1.7.0');src:url('../fonts/academicons.eot?v=1.7.0') format('embedded-opentype'), url('../fonts/academicons.ttf?v=1.7.0') format('truetype'), url('../fonts/academicons.woff?v=1.7.0') format('woff'), url('../fonts/academicons.svg?v=1.7.0#academicons') format('svg');..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By adding a .gz extension to the output path, the data will be automatically compressed with GZip.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched.saveAsJson(\"stylesheets.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to convert and save the dataset to some custom format, please see the recipe on [Extracting title + text from a selected set of URLs](Selected_Title-and-Text.ipynb).\n",
    "\n",
    "For more recipes, please check the [ArchiveSpark documentation](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark",
   "language": "",
   "name": "archivespark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
