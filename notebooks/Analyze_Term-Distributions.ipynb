{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing term / entity distributions in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import de.l3s.archivespark._\n",
    "import de.l3s.archivespark.implicits._\n",
    "import de.l3s.archivespark.enrich.functions._\n",
    "import de.l3s.archivespark.utils.IOUtil\n",
    "import de.l3s.archivespark.specific.warc._\n",
    "import de.l3s.archivespark.specific.warc.enrichfunctions._\n",
    "import de.l3s.archivespark.specific.warc.implicits._\n",
    "import de.l3s.archivespark.specific.warc.specs._\n",
    "import org.apache.hadoop.io.compress.GzipCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In the example, the Web archive dataset will be loaded from local WARC / CDX files, using `WarcCdxHdfsSpec`, but any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) can be used here as well in order to load your records from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"/data/archiveit/ArchiveIt-Collection-2950\"\n",
    "val cdxPath = path + \"/cdx/*.cdx.gz\"\n",
    "val warcPath = path + \"/warc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcCdxHdfsSpec(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering records\n",
    "\n",
    "Embeds are specific to webpages, so we can filter out videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), as well as webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200). In addition to that, we also filter URLs here to only keep pages under *occupylowell.org*, to reduce the size of the dataset for this example.\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual Web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200 && r.surtUrl.startsWith(\"org,occupylowell)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first record in our remaining dataset, we can see that this indeed is of type *text/html* and was *online* (status 200) at the time of crawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"org,occupylowell)/\",\n",
       "    \"timestamp\" : \"20120107063734\",\n",
       "    \"originalUrl\" : \"http://occupylowell.org/\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"NG6JBJ5VEZRU6FRYULHYBRHVNDFCPFR7\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 8020\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL deduplication\n",
    "\n",
    "As we want to consider the number of URLs mentioning a term as the term's frequency, we first need to make sure, every URL is only included once in the dataset. Therefore, we simply decide for the earliest snapshot of each URL. This should be cached, so that it does not need to be recomputed every time the a record is accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val earliest = pages.distinctValue(_.surtUrl) {(a, b) => if (a.time < b.time) a else b}.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16956"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Counting terms\n",
    "\n",
    "To extract the terms of a webpage, we have to keep in mind that a webpage consists of HTML code. Hence, using the `StringContent` Enrich Function would enrich our dataset with this HTML code. To parse the HTML and only keep the text, we provide the `HtmlText` Enrich Function. This can be used to extract the text of a single tag, such as `HtmlText.of(Html.first(\"title\"))` to get the title text of a page. By default, `HtmlText` extracts the entire text of the page though.\n",
    "\n",
    "For more details on the [Enrich Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) provided and their use, please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"org,occupylowell)/forums/member.php?action=profile&uid=1246\",\n",
       "    \"timestamp\" : \"20120131023424\",\n",
       "    \"originalUrl\" : \"http://occupylowell.org/forums/member.php?action=profile&uid=1246\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"UPZFUTUZNFESFJJ5R6LENO3LYEIKS354\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 3680\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"body\" : {\n",
       "          \"text\" : \"Search Member List Calendar Hello There, Guest! Login Register Occupy Lowell Profile of Perbalierinip Perbalierinip (Account not Activated) Registration Date: 01-29-2012 Date of Birth: 01-29-1986 (26 years old) Local Time: 01-31-2012 at 03:34 AM Status: Offline Perbalierinip's Forum Info Joined: 01..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(HtmlText).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn text into terms\n",
    "\n",
    "As a very simple normalization, we convert the text into lowercase, before we split it up into single distinct terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val Terms = LowerCase.of(HtmlText).mapMulti(\"terms\") { text: String => text.split(\"\\\\W+\").distinct }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"org,occupylowell)/forums/member.php?action=profile&uid=1246\",\n",
       "    \"timestamp\" : \"20120131023424\",\n",
       "    \"originalUrl\" : \"http://occupylowell.org/forums/member.php?action=profile&uid=1246\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"UPZFUTUZNFESFJJ5R6LENO3LYEIKS354\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 3680\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"body\" : {\n",
       "          \"text\" : {\n",
       "            \"lowercase\" : {\n",
       "              \"terms\" : [ \"search\", \"member\", \"list\", \"calendar\", \"hello\", \"there\", \"guest\", \"login\", \"register\", \"occupy\", \"lowell\", \"profile\", \"of\", \"perbalierinip\", \"account\", \"not\", \"activated\", \"registration\", \"date\", \"01\", \"29\", \"2012\", \"birth\", \"1986\", \"26\", \"yea..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(Terms).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute term frequencies (number of records / URLs)\n",
    "\n",
    "We can use `.flatMapValues` now to get a plain list of the terms included in the dataset. To get rid of short stopwords like articles, we only keep those terms with a minimum length of 4 characters.\n",
    "\n",
    "For more details on available [ArchiveSpark operations](https://github.com/helgeho/ArchiveSpark/blob/master/docs/Operations.md), please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val terms = earliest.flatMapValues(Terms).filter(_.length >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "member\n",
      "list\n",
      "calendar\n",
      "hello\n",
      "there\n",
      "guest\n",
      "login\n",
      "register\n",
      "occupy\n"
     ]
    }
   ],
   "source": [
    "terms.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we made sure before that every URL is included only once in the dataset and each term is included only once per record, we can simply count the terms, using Spark's `.countByValue`. Finally, we sort the terms by count in descending order (negative count) and save them as [CSV (comma-separated values)](https://en.wikipedia.org/wiki/Comma-separated_values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val counts = terms.countByValue.toSeq.sortBy{case (term, count) => -count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowell,16851\n",
      "occupy,16851\n",
      "list,16730\n",
      "search,16726\n",
      "group,16514\n",
      "powered,16509\n",
      "theme,16481\n",
      "2012,16463\n",
      "time,16456\n",
      "there,16445\n"
     ]
    }
   ],
   "source": [
    "IOUtil.text(\"term_counts.csv\", printTop = 10) { out =>\n",
    "    counts.map{case (term, count) => term + \",\" + count}.foreach(out.println)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `printTop = 10` parameter results in the first 10 lines being printed here for inspection. The `term_counts.csv` that is created in the same folder as this notebook will contain all terms. Now this CSV file can be loaded in a plotting tool of your choice and the term distribution could be plotted in a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting entities\n",
    "\n",
    "Similar to the term frequencies as shown above we can also count the occurrences of [named entities](https://en.wikipedia.org/wiki/Named_entity) in the dataset.\n",
    "\n",
    "An [Enrich Function](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) to extract named entities that we provide with core ArchiveSpark is `Entities`. It uses [Stanford's CoreNLP](https://stanfordnlp.github.io/CoreNLP/) Named Entity Extractor. In order to use it you need to add [`edu.stanford.nlp:stanford-corenlp:3.4.1`](http://central.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.4.1/) with corresponding models to your classpath.\n",
    "\n",
    "Another Enrich Function for more accurate Entity Linking that uses [Yahoo's Fast Entity Linker](https://github.com/yahoo/FEL) (FEL) with ArchiveSpark can be found here: [FEL4ArchiveSpark](https://github.com/helgeho/FEL4ArchiveSpark)\n",
    "\n",
    "For more details on the [Enrich Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md) and their use, please [read the docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"org,occupylowell)/forums/member.php?action=profile&uid=16684\",\n",
       "    \"timestamp\" : \"20120515045601\",\n",
       "    \"originalUrl\" : \"http://occupylowell.org/forums/member.php?action=profile&uid=16684\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"UQ36HQHC7C6W4R2OJJFSAUUHHX4ZWFPM\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 3705\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"body\" : {\n",
       "          \"text\" : {\n",
       "            \"entities\" : {\n",
       "              \"persons\" : [ \"Justin\", \"S.\" ],\n",
       "              \"organizations\" : [ \"Yahoo\" ],\n",
       "              \"locations\" : [ ],\n",
       "              \"dates\" : [ \"Today\", \"11:31\" ]\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(Entities).peekJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Organizations = Entities.mapMulti(\"organizations\", \"distinct\") { values: Seq[String] => values.map(_.toLowerCase).distinct }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"surtUrl\" : \"org,occupylowell)/forums/member.php?action=profile&uid=16684\",\n",
       "    \"timestamp\" : \"20120515045601\",\n",
       "    \"originalUrl\" : \"http://occupylowell.org/forums/member.php?action=profile&uid=16684\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"status\" : 200,\n",
       "    \"digest\" : \"UQ36HQHC7C6W4R2OJJFSAUUHHX4ZWFPM\",\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"meta\" : \"-\",\n",
       "    \"compressedSize\" : 3705\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"body\" : {\n",
       "          \"text\" : {\n",
       "            \"entities\" : {\n",
       "              \"organizations\" : {\n",
       "                \"distinct\" : [ \"yahoo\" ]\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest.enrich(Organizations).peekJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val organizations = earliest.flatMapValues(Organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo\n",
      "lowell\n",
      "board\n",
      "message\n",
      "occupy\n",
      "perbalierinip\n",
      "yahoo\n",
      "lowell\n",
      "board\n",
      "message\n"
     ]
    }
   ],
   "source": [
    "organizations.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note:*\n",
    "\n",
    "*Named Entity Extraction is a pretty expensive operation, depending on the size of the dataset, the following instruction may run for hours or even days.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val counts = organizations.countByValue.toSeq.sortBy{case (term, count) => -count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowell,11837\n",
      "board,11812\n",
      "occupy,11525\n",
      "message,11424\n",
      "yahoo,3940\n",
      "new,94\n",
      "boston,86\n",
      "msn,77\n",
      "messenger,75\n",
      "direct,74\n"
     ]
    }
   ],
   "source": [
    "IOUtil.text(\"organization_counts.csv\", printTop = 10) { out =>\n",
    "    counts.map{case (term, count) => term + \",\" + count}.foreach(out.println)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "The use of `.countByValue` automatically fetches / collects the counts for all available values to the local driver, which may lead to memory issues if the dataset is too big. Instead, the same operation can be implemented by a distributed `.reduceByKey` operation, with a filter to ensure that only values with high counts are fetched in order to avoid memory overruns. This way, also the sorting can be achieved in a distributed fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val termCounts = terms.map(term => (term, 1L)).reduceByKey(_ + _).filter{case (term, count) => count > 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val fetchedTermCounts = termCounts.sortBy{case (term, count) => -count}.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(occupy,16850)\n",
      "(lowell,16850)\n",
      "(list,16729)\n",
      "(search,16725)\n",
      "(group,16513)\n",
      "(powered,16508)\n",
      "(theme,16480)\n",
      "(2012,16462)\n",
      "(time,16455)\n",
      "(there,16444)\n"
     ]
    }
   ],
   "source": [
    "fetchedTermCounts.take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark latest (Spark 2.2.0) dynamic alloc",
   "language": "",
   "name": "archivespark_dynamic"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
