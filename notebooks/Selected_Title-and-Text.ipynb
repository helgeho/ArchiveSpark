{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a corpus with title + text for a selected set of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.archive.webservices.archivespark._\n",
    "import org.archive.webservices.archivespark.functions._\n",
    "import org.archive.webservices.archivespark.specific.warc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this example, the web archive dataset will be loaded from local WARC / CDX files (created in [this recipe](Downloading_WARC_from_Wayback.ipynb)). However, any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) could be used here too, in order to load your records of different types and from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val warcPath = \"/data/helgeholzmann-de.warc.gz\"\n",
    "val cdxPath = warcPath + \"/*.cdx.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcSpec.fromFiles(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering records\n",
    "\n",
    "We can filter out videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), as well as webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200).\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following counts show that we filtered a very big portion, which makes the subsequent processing way more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A peek at the first record of the filtered dataset (in pretty JSON format) shows that it indeed consists of HTML pages with successful status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152652\",\n",
       "        \"digest\" : \"sha1:HCHVDRUSN7WDGNZFJES2Y4KZADQ6KINN\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 2087,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant records based on a given set of URLs\n",
    "\n",
    "We now load the desired URLs into a Spark RDD. In this example, the list of URLs (here only one) is specified in code, but it could also be loaded from a file or other sources. These are then converted into the canonical SURT format, using a function from the *Sparkling* library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val urls = Set(\"https://www.helgeholzmann.de/publications\").map(org.archive.webservices.sparkling.util.SurtUtil.fromUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set(de,helgeholzmann)/publications)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this data available to Spark (across all nodes of our computing environment), we use broadcast: (if the set of URLs it too big, a `join` operation should be used here instead of a broadcast, for an example see the recipe on [Extracting embedded resources from webpages](Extracting_Embeds.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val selectedUrls = sc.broadcast(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the pages in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filtered = pages.filter(r => selectedUrls.value.contains(r.surtUrl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich the dataset with the desired information (title + text)\n",
    "\n",
    "To access the content of an HTML page, ArchiveSpark comes with an `Html` Enrichment Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152831\",\n",
       "        \"digest\" : \"sha1:XRVCBHVKAC6NQ4N24OCF4S2ABYUOJW3H\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/publications\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/publications\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 4280,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"html\" : \"<html>\\r\\n<head>\\r\\n    <title>Helge Holzmann - @helgeho</title>\\r\\n    <link rel=\\\"shortcut icon\\\" href=\\\"/images/favicon.png\\\">\\r\\n    <link rel=\\\"stylesheet\\\" href=\\\"/css/font-awesome.min.css\\\">\\r\\n    <link rel=\\\"stylesheet\\\" href=\\\"/css/academicons.min.css\\\">\\r\\n    <link rel=\\\"stylesheet\\\" href=\\\"/css..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Html).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by default `Html` extracts the body of the page. To customize this, it provides different ways to specify which tags to extract:\n",
    "* `Html.first(\"title\")` will extract the (first) title tag instead\n",
    "* `Html.all(\"a\")` will extract all anchors / hyperlinks (the result is a list instead of a single item)\n",
    "* `Html(\"p\", 2)` will extract the third paragraph of the page (index 2 = third match)\n",
    "\n",
    "Fore more details as well as additional [Enrichment Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md), please read the [docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152831\",\n",
       "        \"digest\" : \"sha1:XRVCBHVKAC6NQ4N24OCF4S2ABYUOJW3H\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/publications\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/publications\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 4280,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"title\" : \"<title>Helge Holzmann - @helgeho</title>\"\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Html.first(\"title\")).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are only interested in the text without the HTML tags (`<title>`), we need to use the `HtmlText` Enrichment Function. This, by default, depends on the default version of `Html`, hence it would extract the text of the body, i.e., the complete text of the page. In order to change this dependency to get only the title, we can use the `.on`/`.of` method that all Enrichment Functions provide. Now we can give this new Enrichment Function a name (`Title`) to reuse it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Title = HtmlText.of(Html.first(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152831\",\n",
       "        \"digest\" : \"sha1:XRVCBHVKAC6NQ4N24OCF4S2ABYUOJW3H\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/publications\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/publications\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 4280,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"title\" : {\n",
       "                    \"text\" : \"Helge Holzmann - @helgeho\"\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Title).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the title, we would also like to have the full text of the page. This will be our final dataset, so we assign it to a new variable (`enriched`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val BodyText = HtmlText.of(Html.first(\"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val enriched = filtered.enrich(Title).enrich(BodyText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"record\" : {\n",
       "        \"redirectUrl\" : \"-\",\n",
       "        \"timestamp\" : \"20190528152831\",\n",
       "        \"digest\" : \"sha1:XRVCBHVKAC6NQ4N24OCF4S2ABYUOJW3H\",\n",
       "        \"originalUrl\" : \"https://www.helgeholzmann.de/publications\",\n",
       "        \"surtUrl\" : \"de,helgeholzmann)/publications\",\n",
       "        \"mime\" : \"text/html\",\n",
       "        \"compressedSize\" : 4280,\n",
       "        \"meta\" : \"-\",\n",
       "        \"status\" : 200\n",
       "    },\n",
       "    \"payload\" : {\n",
       "        \"string\" : {\n",
       "            \"html\" : {\n",
       "                \"title\" : {\n",
       "                    \"text\" : \"Helge Holzmann - @helgeho\"\n",
       "                },\n",
       "                \"body\" : {\n",
       "                    \"text\" : \"Home Research Publications Private Projects Contact Helge Holzmann I am a researcher and PhD candidate at the L3S Research Center in Hannover, Germany. My main research inte..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save the created corpus\n",
    "\n",
    "The dataset can either be saves in JSON format as shown in the peek operations above, which is supported by ArchiveSpark, or it can be converted to some custom format and saved the raw text (using Spark's `saveAsTextFile`): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as JSON\n",
    "By adding a `.gz` extension to the path, ArchiveSpark will automatically compress the output using Gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched.saveAsJson(\"/data/title-text_dataset.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in a custom format\n",
    "\n",
    "The Enrichment Functions (`Title` and `BodyText`) can be used as accessors to read the corresponding values, so we can create a tab separated format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tsv = enriched.map{r =>\n",
    "    // replace tab and newlines with a space\n",
    "    val title = r.valueOrElse(Title, \"\").replaceAll(\"[\\\\t\\\\n]\", \" \")\n",
    "    val text = r.valueOrElse(BodyText, \"\").replaceAll(\"[\\\\t\\\\n]\", \" \")\n",
    "    // concatenate URL, timestamp, title and text with a tab\n",
    "    Seq(r.originalUrl, r.timestamp, title, text).mkString(\"\\t\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://www.helgeholzmann.de/publications\t20190528152831\tHelge Holzmann - @helgeho\tHome Research Publications Private Projects Contact Helge Holzmann I am a researcher and PhD candidate at the L3S Research Center in Hannover, Germany. My main research interest is on Web archives and related topics, such as big data processing, graph analysis and information retrieval. @helgeho on Twitter helgeho on GitHub Helge on arXiv Email me! Publications 2017 short H. Holzmann, Emily Novak Gustainis and Vinay Goel. Universal Distant Reading through Metadata Proxies with ArchiveSpark. 5th IEEE International Conference on Big Data (BigData). Boston, MA, USA. December 2017. H. Holzmann, W. Nejdl and A. Anand. Exploring Web Archives Through Temporal Anchor Texts. 7th International ACM C..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv.peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv.saveText(\"/data/title-text_dataset.tsv.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark",
   "language": "",
   "name": "archivespark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
