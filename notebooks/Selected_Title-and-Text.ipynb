{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a corpus with title + text for a selected set of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import de.l3s.archivespark._\n",
    "import de.l3s.archivespark.implicits._\n",
    "import de.l3s.archivespark.enrich.functions._\n",
    "import de.l3s.archivespark.specific.warc._\n",
    "import de.l3s.archivespark.specific.warc.enrichfunctions._\n",
    "import de.l3s.archivespark.specific.warc.implicits._\n",
    "import de.l3s.archivespark.specific.warc.specs._\n",
    "import org.apache.hadoop.io.compress.GzipCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In the example, the Web archive dataset will be loaded from local WARC / CDX files, using `WarcCdxHdfsSpec`, but any other [Data Specification (DataSpec)](https://github.com/helgeho/ArchiveSpark/blob/master/docs/DataSpecs.md) can be used here as well in order to load your records from different local or remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val path = \"/data/archiveit/ArchiveIt-Collection-2950\"\n",
    "val cdxPath = path + \"/cdx/*.cdx.gz\"\n",
    "val warcPath = path + \"/warc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val records = ArchiveSpark.load(WarcCdxHdfsSpec(cdxPath, warcPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering records\n",
    "\n",
    "We can filter out videos, images, stylesheets and any other files except for webpages ([mime type](https://en.wikipedia.org/wiki/Media_type) *text/html*), as well as webpages that were unavailable when they were crawled either ([status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) == 200).\n",
    "\n",
    "*It is important to note that this filtering is done only based on metadata, so up to this point ArchiveSpark does not even touch the actual Web archive records, which is the core efficiency feature of ArchiveSpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pages = records.filter(r => r.mime == \"text/html\" && r.status == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following counts show that we filtered a very big portion, which makes the subsequent processing way more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52922792"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16062674"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A peek at the first record of the filtered dataset (in pretty JSON format) shows that it indeed consists of HTML pages with successful status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111222043804\",\n",
       "    \"digest\" : \"YVOEIYJ45I7QNNFBQTCPKIQAQJIE4B46\",\n",
       "    \"originalUrl\" : \"http://english.cntv.cn/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"surtUrl\" : \"cn,cntv,english)/program/newsupdate/20110504/109544.shtml\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"compressedSize\" : 10855,\n",
       "    \"meta\" : \"-\",\n",
       "    \"status\" : 200\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant records based on a given set of URLs\n",
    "\n",
    "In this example, the desired URLs are stored in a comma separated file format along with some additional information. So we need to load these records, split them by comma and select the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val urlRecords = sc.textFile(\"relevant_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True,http://15ocroatia.org/,2950,31104006172,system,2012-01-23 18:59:27+00:00,False,200,205223,2012-02-09 03:19:25+00:00,system,2013-02-15 01:55:53+00:00,,,,,,,,,,,True,1156,normal,http://15ocroatia.org/,True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlRecords.peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val urls = urlRecords.map(_.split(\",\")).map(_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://15ocroatia.org/"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls.peek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter our Web archive dataset based on these URLs, we convert them to the canonical SURT format in order to get rid of slight, negligible differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val surtUrls = urls.map(de.l3s.archivespark.utils.SURT.fromUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org,15ocroatia)/"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surtUrls.peek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally collect these URLs, convert them to a set and broadcast this across our computing environment (if the set of URLs it too big, a `join` operation should be used here instead of a broadcast, for an example see the recipe on [Extracting embedded resources from webpages](Extract_Embeds.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val selectedUrls = sc.broadcast(surtUrls.collect.toSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the pages in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val filtered = pages.filter(r => selectedUrls.value.contains(r.surtUrl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38301"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich the dataset with the desired information (title + text)\n",
    "\n",
    "To access the content of an HTML page, ArchiveSpark comes with an `Html` Enrich Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111220013011\",\n",
       "    \"digest\" : \"RJKJCKKSWCM7QDOC6U2FLDKKVCOJNMU5\",\n",
       "    \"originalUrl\" : \"http://blog.alexanderhiggins.com/\",\n",
       "    \"surtUrl\" : \"com,alexanderhiggins,blog)/\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"compressedSize\" : 43070,\n",
       "    \"meta\" : \"-\",\n",
       "    \"status\" : 200\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"body\" : \"<body> \\n <!--\\r\\nFind out if this theme has the preset mode deactivated. If its not deactivated then lets show off a bit. Party time!\\r\\n~~~~~~~~~~~~~~~~~~~~~~~~ ~~~ --> \\n <!--\\r\\nThe High Bar\\r\\n~~~~~~~~~~~~~~~~~~~~~~~~ ~~~ --> \\n <script src=\\\"http://blog.alexanderhiggins.com/msg.js\\\" type=\\\"text/javascript\\\">\\r\\n</script> \\n <center class=\\\"homealert\\\"> \\n  <script type=\\\"tex..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Html).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by default `Html` extracts the body of the page. To customize this, it provides different ways to specify which tags to extract:\n",
    "* `Html.first(\"title\")` will extract the (first) title tag instead\n",
    "* `Html.all(\"a\")` will extract all anchors / hyperlinks (the result is a list instead of a single item)\n",
    "* `Html(\"p\", 2)` will extract the third paragraph of the page (index 2 = third match)\n",
    "\n",
    "Fore more details as well as additional [Enrich Functions](https://github.com/helgeho/ArchiveSpark/blob/master/docs/EnrichFuncs.md), please read the [docs](https://github.com/helgeho/ArchiveSpark/blob/master/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111220013011\",\n",
       "    \"digest\" : \"RJKJCKKSWCM7QDOC6U2FLDKKVCOJNMU5\",\n",
       "    \"originalUrl\" : \"http://blog.alexanderhiggins.com/\",\n",
       "    \"surtUrl\" : \"com,alexanderhiggins,blog)/\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"compressedSize\" : 43070,\n",
       "    \"meta\" : \"-\",\n",
       "    \"status\" : 200\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"title\" : \"<title>Alexander Higgins Blog - The Latest Buzz, Analysis, and News Without the Snooze!</title>\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Html.first(\"title\")).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are only interested in the text without the HTML tags (`<title>`), we need to use the `HtmlText` Enrich Function. This, by default, depends on the default version of `Html`, hence it would extract the text of the body, i.e., the complete text of the page. In order to change this dependency to get only the title, we can use the `.on`/`.of` method that all Enrich Functions provide. Now we can give this new Enrich Function a name (`Title`) to reuse it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Title = HtmlText.of(Html.first(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111220013011\",\n",
       "    \"digest\" : \"RJKJCKKSWCM7QDOC6U2FLDKKVCOJNMU5\",\n",
       "    \"originalUrl\" : \"http://blog.alexanderhiggins.com/\",\n",
       "    \"surtUrl\" : \"com,alexanderhiggins,blog)/\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"compressedSize\" : 43070,\n",
       "    \"meta\" : \"-\",\n",
       "    \"status\" : 200\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"title\" : {\n",
       "          \"text\" : \"Alexander Higgins Blog - The Latest Buzz, Analysis, and News Without the Snooze!\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.enrich(Title).peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the title, we would also like to have the full text of the page. This will be our final dataset, so we assign it to a new variable (`enriched`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val enriched = filtered.enrich(Title).enrich(HtmlText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"record\" : {\n",
       "    \"redirectUrl\" : \"-\",\n",
       "    \"timestamp\" : \"20111220013011\",\n",
       "    \"digest\" : \"RJKJCKKSWCM7QDOC6U2FLDKKVCOJNMU5\",\n",
       "    \"originalUrl\" : \"http://blog.alexanderhiggins.com/\",\n",
       "    \"surtUrl\" : \"com,alexanderhiggins,blog)/\",\n",
       "    \"mime\" : \"text/html\",\n",
       "    \"compressedSize\" : 43070,\n",
       "    \"meta\" : \"-\",\n",
       "    \"status\" : 200\n",
       "  },\n",
       "  \"payload\" : {\n",
       "    \"string\" : {\n",
       "      \"html\" : {\n",
       "        \"title\" : {\n",
       "          \"text\" : \"Alexander Higgins Blog - The Latest Buzz, Analysis, and News Without the Snooze!\"\n",
       "        },\n",
       "        \"body\" : {\n",
       "          \"text\" : \"Alexander Higgins Blog The Latest Buzz, Analysis, and News Without the Snooze! Home Headlines Authors About Subscribe, Friend or Follow Advertise Economy Environment Headlines Health Member Submitted Projects Society Technology ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched.peekJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save the created corpus\n",
    "\n",
    "The dataset can either be saves in JSON format as shown in the peek operations above, which is supported by ArchiveSpark, or it can be converted to some custom format and saved the raw text (using Spark's `saveAsTextFile`): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as JSON\n",
    "By adding a `.gz` extension to the path, ArchiveSpark will automatically compress the output using Gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched.saveAsJson(\"title-text_dataset.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in a custom format\n",
    "\n",
    "The Enrich Functions (`Title` and `HtmlText`) can be used as accessors to read the corresponding values, so we can create a tab separated format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val tsv = enriched.map{r =>\n",
    "    // replace tab and newlines with a space\n",
    "    val title = r.valueOrElse(Title, \"\").replaceAll(\"[\\\\t\\\\n]\", \" \")\n",
    "    val text = r.valueOrElse(HtmlText, \"\").replaceAll(\"[\\\\t\\\\n]\", \" \")\n",
    "    // concatenate URL, timestamp, title and text with a tab\n",
    "    Seq(r.originalUrl, r.timestamp, title, text).mkString(\"\\t\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://blog.alexanderhiggins.com/\t20111220013011\tAlexander Higgins Blog - The Latest Buzz, Analysis, and News Without the Snooze!\tAlexander Higgins Blog The Latest Buzz, Analysis, and News Without the Snooze! Home Headlines Authors About Subscribe, Friend or Follow Advertise Economy Environment Headlines Health Member Submitted Projects Society Technology The Alexander Higgins Show Uncategorized US Videos Web Development World Email Subscription Comments Posts table, td, th { vertical-align:top !important; } .itemblock { /*height:85px;*/ } .pinned a { background-color:#FFFFCC} .homeheadline, .pinned a { display:block;padding:5px; border:1px solid #cccccc; margin:3px; padding: 3px; width:636px; font-weight:bold; } New Feature: Click Here To Submit A Story Have a feed you ..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv.peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsv.saveAsTextFile(\"title-text_dataset.tsv.gz\", classOf[GzipCodec])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchiveSpark latest (Spark 2.2.0) dynamic alloc",
   "language": "",
   "name": "archivespark_dynamic"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
